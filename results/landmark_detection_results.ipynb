{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d6c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow before transformers with logs deactivated to avoid printing tf logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "if not tf.executing_eagerly():\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
    "from tqdm import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b96f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = torch.tensor([5, 6, 7])\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59735b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoFilteredMaskDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, extractor, target_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory containing images.\n",
    "            annotation_file (str): Path to COCO-format JSON file.\n",
    "            extractor: Segformer feature extractor.\n",
    "            target_classes (torch.Tensor or list[int]): List or tensor of category IDs to keep as foreground (label=1).\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.extractor = extractor\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "        # Store target category IDs as a tensor for fast comparison\n",
    "        self.target_cat_ids = torch.tensor(target_classes) if not isinstance(target_classes, torch.Tensor) else target_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.image_dir, img_info[\"file_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        height, width = img_info[\"height\"], img_info[\"width\"]\n",
    "        mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        for ann in anns:\n",
    "            if ann[\"iscrowd\"] == 0:\n",
    "                ann_mask = self.coco.annToMask(ann)\n",
    "                category_id = ann[\"category_id\"]\n",
    "                # Apply the category_id to the mask (overwrite existing values)\n",
    "                mask[ann_mask == 1] = category_id\n",
    "\n",
    "        encoded = self.extractor(image, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": encoded[\"pixel_values\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"image\": image\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco_split(dataset_dir, split_name, feature_extractor):\n",
    "    image_dir = os.path.join(dataset_dir, split_name)\n",
    "    annotation_path = os.path.join(dataset_dir, split_name, \"_annotations.coco.json\")\n",
    "    dataset = CocoFilteredMaskDataset(image_dir, annotation_path, feature_extractor, target_classes)\n",
    "    return DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=lambda x: x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, extractor, image, device):\n",
    "    pixel_values = extractor(image, return_tensors='pt').pixel_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(pixel_values).logits\n",
    "        logits = F.interpolate(\n",
    "            logits,\n",
    "            size=image.size[::-1],  # (width, height) â†’ (height, width)\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "    labels = torch.argmax(logits.squeeze(), dim=0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0481d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_split(model, loader, split_name, feature_extractor, device):\n",
    "    metric = evaluate.load(\"mean_iou\")\n",
    "    for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "        sample = batch[0]\n",
    "        image = sample[\"image\"]\n",
    "        label = sample[\"labels\"].to(device)\n",
    "\n",
    "        prediction = predict(model, feature_extractor, image, device)\n",
    "        binary_prediction = torch.isin(prediction, target_classes).long()\n",
    "        predictions = binary_prediction.unsqueeze(0).cpu().numpy()\n",
    "        references = label.unsqueeze(0).numpy()\n",
    "\n",
    "        tqdm.write(f\"predictions shape: {predictions.shape}\")\n",
    "        tqdm.write(f\"references shape: {references.shape}\")\n",
    "\n",
    "        metric.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references\n",
    "        )\n",
    "        tqdm.write(str(metric.compute(num_labels=2, ignore_index=None)))\n",
    "\n",
    "    results = metric.compute(num_labels=NUM_CLASSES)\n",
    "    print(f\"\\n{split_name.capitalize()} Metrics:\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load segformer model for evaluation\n",
    "model_name = \"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\"\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(model_name)\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(\n",
    "    os.path.dirname((os.getcwd())), \"dataset/segmentation/waymo_landmark_segmentation.v1-v1_poles.coco-segmentation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = load_coco_split(dataset_dir, \"train\", feature_extractor)\n",
    "val_loader = load_coco_split(dataset_dir, \"valid\", feature_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5028bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_split(model, train_loader, \"train\", feature_extractor, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_split(model, val_loader, \"valid\", feature_extractor, 'cpu')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waymo_env_v4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
